device_train_batch_size: 8
device_eval_batch_size: 8
gradient_accumulation_steps: 16
num_train_epochs: 1
weight_decay: 0.1
warmup_ratio: 0.05
lr_scheduler_type: 'polynomial'
learning_rate: 1.5e-3
adam_epsilon: 1e-8
optim: adamw_hf
max_grad_norm: 1